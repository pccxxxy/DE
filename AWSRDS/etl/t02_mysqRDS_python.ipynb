{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dffd209a-6939-4e5d-9c7a-e2589634ded7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a9f0907-c89e-4dcc-a44c-e30e9000ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Phase\n",
    "def extract_from_local(csv_file):\n",
    "    \"\"\"Extracts data from a CSV file.\"\"\"\n",
    "    return pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "\n",
    "def extract_from_db(db_url, username, password, db_name):\n",
    "    \"\"\"Extracts data from a remote db hosted on AWS RDS.\"\"\"\n",
    "    \n",
    "    # Format the connection string for SQLAlchemy\n",
    "    db_connection_str = f'mysql+pymysql://{username}:{password}@{db_url}/{db_name}'\n",
    "    \n",
    "    # Create a database connection using SQLAlchemy\n",
    "    db_connection = create_engine(db_connection_str)\n",
    "    \n",
    "    # Example: Loading data from a table\n",
    "    query = \"SELECT * FROM athlete_events\"\n",
    "    data = pd.read_sql(query, db_connection)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Transform Phase\n",
    "def transform(data):\n",
    "    \"\"\"Cleans and transforms the data.\"\"\"\n",
    "    # Remove rows with missing values\n",
    "    transformed_data = data.dropna()\n",
    "\n",
    "    # Convert all column names to lowercase\n",
    "    transformed_data.columns = [c.lower() for c in transformed_data.columns]\n",
    "\n",
    "    \n",
    "    columns_to_convert = ['age', 'height', 'weight']\n",
    "\n",
    "    # Drop rows where 'age', 'height', or 'weight' have 'NA' values\n",
    "    transformed_data = transformed_data[~transformed_data[columns_to_convert].isin(['NA', '']).any(axis=1)]\n",
    "\n",
    "    # Convert the columns to numeric type for calculation\n",
    "    transformed_data[columns_to_convert] = transformed_data[columns_to_convert].astype(float)\n",
    "\n",
    "    # Calculate averages for specified columns\n",
    "    averages = {\n",
    "        'avg_age': transformed_data['age'].mean(),\n",
    "        'avg_height': transformed_data['height'].mean(),\n",
    "        'avg_weight': transformed_data['weight'].mean()\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame for the statistical data\n",
    "    stats_data = pd.DataFrame([averages])\n",
    "\n",
    "    # Calculate averages for specified columns grouped by 'year'\n",
    "    grouped_data = transformed_data.groupby('year')[['age', 'height', 'weight']].mean().reset_index()\n",
    "    \n",
    "    # Rename columns to indicate these are averages\n",
    "    grouped_data.rename(columns={\n",
    "        'age': 'avg_age',\n",
    "        'height': 'avg_height',\n",
    "        'weight': 'avg_weight'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return transformed_data, stats_data, grouped_data\n",
    "\n",
    "# Load Phase\n",
    "def load_to_db(db_url, username, password, db_name, load_df, table_name, if_exists='replace', index=False):\n",
    "    \"\"\"\n",
    "    Loads data to a remote database hosted on AWS RDS using bulk insert.\n",
    "    \n",
    "    Parameters:\n",
    "        db_url (str): Database URL, typically the RDS endpoint.\n",
    "        username (str): Username for the RDS database.\n",
    "        password (str): Password for the RDS database.\n",
    "        db_name (str): Database name.\n",
    "        load_df (DataFrame): DataFrame to load into the database.\n",
    "        table_name (str): Name of the table where data will be inserted.\n",
    "        if_exists (str): Behavior if the table already exists - 'fail', 'replace', 'append'. Default is 'replace'.\n",
    "        index (bool): Whether to write DataFrame index as a column. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the connection string for SQLAlchemy\n",
    "    db_connection_str = f'mysql+pymysql://{username}:{password}@{db_url}/{db_name}'\n",
    "    \n",
    "    # Create a database connection using SQLAlchemy\n",
    "    db_engine = create_engine(db_connection_str)\n",
    "    \n",
    "    # Number of rows before insertion\n",
    "    rows_before = pd.read_sql(f\"SELECT COUNT(*) as count FROM {table_name}\", db_engine).iloc[0]['count']\n",
    "    \n",
    "    # Use pandas to_sql function to perform bulk insert\n",
    "    load_df.to_sql(table_name, db_engine, if_exists=if_exists, index=index, method='multi')\n",
    "\n",
    "    # Number of rows after insertion\n",
    "    rows_after = pd.read_sql(f\"SELECT COUNT(*) as count FROM {table_name}\", db_engine).iloc[0]['count']\n",
    "    \n",
    "    # Calculate the number of rows inserted\n",
    "    rows_inserted = rows_after - rows_before if if_exists != 'replace' else len(load_df)\n",
    "\n",
    "    print(f\"Data successfully loaded to {table_name}. Rows inserted: {rows_inserted}.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecbcd352-f23d-406e-a0d6-d79129a33bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extract Completed, Length of Source DataFrame: 99998\n",
      " \n",
      "Data Transformation Completed:\n",
      "Length of Transformed DataFrame: 75045\n",
      "Length of Stats DataFrame: 1\n",
      "Length of Grouped DataFrame: 35\n",
      " \n",
      "Data successfully loaded to trans_athlete_events. Rows inserted: 75045.\n",
      "Data Load Completed\n"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file\n",
    "# csv_file_path = 'practice_dataset/athlete_events.csv'\n",
    "RDS_URL = 'database-1.cvi0ecu6mury.ap-southeast-2.rds.amazonaws.com'\n",
    "USERNAME = 'admin'\n",
    "PASSWORD = '111232***'\n",
    "DB_NAME = 'demo_db'\n",
    "\n",
    "\n",
    "# Running the ETL process\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Extract\n",
    "    # df = extract(csv_file_path)\n",
    "    df = extract_from_db(RDS_URL, USERNAME, PASSWORD, DB_NAME)\n",
    "    print(f\"Data Extract Completed, Length of Source DataFrame: {len(df)}\")\n",
    "    print(f\" \")\n",
    "\n",
    "\n",
    "    # Step 2: Transform\n",
    "    transformed_df, stats_df, grouped_df = transform(df)\n",
    "\n",
    "    # Print the lengths of the DataFrames\n",
    "    print(f\"Data Transformation Completed:\")\n",
    "    print(f\"Length of Transformed DataFrame: {len(transformed_df)}\")\n",
    "    print(f\"Length of Stats DataFrame: {len(stats_df)}\")\n",
    "    print(f\"Length of Grouped DataFrame: {len(grouped_df)}\")\n",
    "    print(f\" \")\n",
    "\n",
    "    \n",
    "    # Step 3: Load\n",
    "    target_table = 'trans_athlete_events'\n",
    "    load_to_db(RDS_URL, USERNAME, PASSWORD, DB_NAME, transformed_df, target_table)\n",
    "    print(f\"Data Load Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a0a11-92e8-425f-9609-ef27dfaa1567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6abb1b-e8ab-4b6d-b061-869a71a1fe29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475476cf-a9e6-4f24-b178-414b3bfdd279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
